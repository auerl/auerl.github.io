<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>https://auerl.github.io on https://auerl.github.io</title>
    <link>/</link>
    <description>Recent content in https://auerl.github.io on https://auerl.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Building a scalable GNSS-IMU sensor fusion pipeline using Docker, Luigi, ECS and S3</title>
      <link>/post/tracktics-data-pipeline/</link>
      <pubDate>Mon, 18 Mar 2019 23:05:47 +0100</pubDate>
      
      <guid>/post/tracktics-data-pipeline/</guid>
      <description>

&lt;p&gt;At TRACKTICS we aim to bring sophisticated tactical analytics and performance
stats (which have so far been reserved to elite clubs and players) to the lower
football leagues, by developing a GNSS-IMU based tracking solution that fits
the tight budgets of an average amatuer player or coach, but is yet accurate
enough to provide valuable insights for injury prevention and training control.&lt;/p&gt;

&lt;p&gt;One of the key ideas that we have put forward in order to achieve this goal was to
repurpose methods from the field of sensor fusion, commonly used in fields like
autonomous driving or robotics, for the use case of football data enrichment,
thus accounting for potential shortcomings in raw data quality caused by
inexpensive hardware components and allowing our system to achieve state-of-the
art accuracy, akin to that of much more expensive systems.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;data.png&#34; width=800 alt=&#34;Luigi&#39;s Web UI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the following article I am summarizing TRACKTICS&amp;rsquo; (or more accurately, its
tech team&amp;rsquo;s) journey towards achieving this goal. While the focus of this
article is on on our highly-available and fault tolerant data analysis
pipeline (&amp;ldquo;pipeline&amp;rdquo; in the following) I will also touch upon our core
sensor fusion algorithm and our &amp;ldquo;continuous evaluation&amp;rdquo; framework.&lt;/p&gt;

&lt;h2 id=&#34;system-overview&#34;&gt;System overview&lt;/h2&gt;

&lt;p&gt;Before going into detail about the &amp;ldquo;pipeline&amp;rdquo; I would like to go one step
backwards, and start with delineating the most important business requirements
driving the work of TRACKTICS Tech-team. Most importantly, the target system
architecture was meant to &amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;provide high-quality estimation result, superior to what can be achieved
by off-the-shelf fitness TRACKERS and comparable to that of professional
GPS-based football tracking solutions by Catapult or GPSports.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;be optimized for a strong user experience, through exciting Apps, and a
strong emphasis on wearing comfort - in particular we wanted to get rid of
the widely despised &amp;ldquo;Sports-Bras&amp;rdquo; which are used by practically any competitor
system.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;be affordable to the average youth and amateuer player or semi-professional,
and ambitioned Coaches from European 4th leagues and downwards.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;allow for rapid scaling to tens and hundreds of thousand users.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before I joined TRACKTICS Tech team in 2016, TRACKTICS had already developed
a powerful GNSS-IMU based sensor hardware tailored for the for the logging of
movements of a football player during game or training sessions a first version
of a Web App that allowed one to visualize basic metrics inferred using an Extended
Kalman Filter (executed in an offline manner) to combine the GNSS IMU and
Compass readings from the tracker. Yet the&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;system.png&#34; width=800&gt;&lt;/p&gt;

&lt;h2 id=&#34;slam-for-football-analytics&#34;&gt;SLAM for football analytics&lt;/h2&gt;

&lt;p&gt;To improve upon the estimation results that could be achieved with our first
prototype algorithm, we teamed up with sensor fusion experts from
&lt;a href=&#34;http://www.knowtion.de/&#34; target=&#34;_blank&#34;&gt;Knowtion&lt;/a&gt; to develop a new and more powerful
estimation approach based on the the &lt;a href=&#34;https://bitbucket.org/gtborg/gtsam/&#34; target=&#34;_blank&#34;&gt;GTSAM
library&lt;/a&gt; - a toolkit commonly used
for SLAM (Similtaneous Localization and Mapping) applications and other
types of robotics and vision. GTSAM is implemented in C++, uses Factor Graphs
and Bayes Networks as its underlying compute paradigm and provides a modern
IMU preintegration scheme and convenient Python bindings for rapid prototyping.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;factorgraph.png&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;In the initial stages of designing our algorithm we were faced with the question
whether to go for near real-time online estimation, which is supported by
GTSAM&amp;rsquo;s iSAM2 module, which &amp;hellip; in small increments. Yet, since initial
experiments showed slightly better estimation results for the more conventional
batch processing approach, that allows to better account for the estimation of
global parameters (such as the local magnetic field at the recording location or
hardware-related corrections such IMU and magnetometer biases and scaling
factors). Thus, in light of our premisse to ensure as high quality estimation
results as possible&amp;hellip;&lt;/p&gt;

&lt;p&gt;Our sensor fusion algorithms are compute intensive Therefore the task at hand
doesn&amp;rsquo;t clearly fit, but since our sensor fusion algorithm involves. Online vs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Relatively generous latency requirements of a few minutes. Users can see the
estimation results after 3-5 minutes. As our user journey involves a few steps
where the user is asked to input meta information about their data (e.g. adding
geographic coordinates of the football pitch or events) the perceived waiting
time could be reduced.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As our sensor hardware has relatively narrow price and dimension constraints
and regulatory reasons (the current regulations on sensory hardware that may
be applied during &amp;hellip; games, put forward by FIFA, real-time capabilities that
would allow the) TRACKTICS opted for an approach where data analysis is entirely
performed in the cloud.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;stitching-it-together&#34;&gt;Stitching it together&lt;/h2&gt;

&lt;!---
&gt;DESCRIBE THAT WE HAD TO COME UP WITH SOME WAY TO DO
&gt;WRITE A LITTLE BIT ABOUT BATCH PROCESSING VS STREAM PROCESSING
--&gt;

&lt;p&gt;As soon as we were happy with the overall performance of our algorithm w.r.t.
the groundtruth data that we had collected, we were facing the challenge to
run. In the rrecent years, there have been a number of blog posts (see, for
example, the interesting
&lt;a href=&#34;http://tech.adroll.com/blog/data/2015/09/22/data-pipelines-docker.html&#34; target=&#34;_blank&#34;&gt;article&lt;/a&gt;
by Ville Tuulos from AdRoll, which was one of the inspirations for our
pipeline) describing architectures, that are meant to handle quite similar
batch processing type workloads as ours. Interestingly, these architectures
often converge to a surprisingly similar configuration, based on the
following main building blocks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;An object storage solution like Amazons S3 (Simple Storage Solution) which is
used like a cloud-native equivalent to a computers file system.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Docker for dependency management, that allows for a clearer separation of
domains between application development and deployment/operation of the
batch processing workflow.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A container orchestrator (like Kubernetes or Amazon EC2 Container Service)
combined with some Auto-Scaling solution (with a custom scaling logic) for
resource management.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A workflow manager to stich together the workflow (like Spotify&amp;rsquo;s
&lt;a href=&#34;https://github.com/spotify/luigi&#34; target=&#34;_blank&#34;&gt;Luigi&lt;/a&gt; or AirBnB&amp;rsquo;s
&lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34;&gt;Airflow&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In our case, we also required the following components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A resilient messaging system (like Amazons Simple Queuing System
or Apache Kafka) as a fault-tolerant way to handle our job queue.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A REST API as a communication link between our batch processing pipeline and
the various client apps that allow our user to view their data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;pipeline.png&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;In such an architecture, each (short-lived) Docker container is responsible for
running one to a few modules of a batch processing job. For example, in the
simplemost case there might be one container running our sensor fusion algorithm
and one container running some module to estimate soccer-related metrics (like
the number of sprints, etc.). Each container can be seen idempotent function which is applied
to a immutable piece of input data (a file on S3).&lt;/p&gt;

&lt;p&gt;Docker comes in handy as it allows for an easy resource. Docker is mainstream,
and can be installed within an instance on a developers laptop, this facilitates
&lt;a href=&#34;https://medium.com/machine-words/separation-of-concerns-1d735b703a60&#34; target=&#34;_blank&#34;&gt;separation of
concerns&lt;/a&gt;
which, in our case, means that different modules along our pipeline are
maintained by different people. The fact that building and running a Docker
container locally is extremely easy, ensures that application developers can
independently test their builds. If their containers runs locally its guaranteed
to run in the production environment. As S3 is so cheap to use it is even
affordable to store intermediate results upon demand, which simplifies detailed
inspection of the data for troubleshooting.&lt;/p&gt;

&lt;p&gt;As a workflow manager we opted for &lt;a href=&#34;https://github.com/spotify/luigi&#34; target=&#34;_blank&#34;&gt;Luigi&lt;/a&gt;
open-sourced by by Spotify in 2015. Spotify doesn&amp;rsquo;t comprise as many features
compared to the more common alternative Airflow (by AirBnB), but is much more
lightweight and easier to set up, which was the main reason for our choice.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;luigi.png&#34; width=800 alt=&#34;Luigi&#39;s Web UI&#34; /&gt;&lt;/p&gt;

&lt;!---
&gt;DESCRIPTION OF LUIGI VS AIRFLOW VS ...
&gt;DIRECTED ACYCLIC GRAPHS
&gt;CONTAINER ORCHESTRATION (ECS, FARGATE, SOME WORDS ON KUBERNETES AND WHY WE DONT USE IT)
&gt;EC2 INSTANCE CONFIGURATION (C4 INSTANCES)
&gt;THE COSTS PER SESSION 
&gt;SCALING APPROACH, SPOT INSTANCES
Docker lends itself, as an ideal technology that allows... We use the new c4.xlarge instances




## Performance considerations

&gt;WE IMPLEMENTED PROTOTYPE VERSIONS OF OUR ALGORITHMS IN C++ AND
&gt;WE TRANSLATED A KALMAN SMOOTHER FROM PYTHON TO C++ ARMADILLO
&gt;WE TRANSLATED OUR IMU PREPROCESSOR USING CYTHON
--&gt;

&lt;h2 id=&#34;ci-cd-and-testing-strategy&#34;&gt;CI/CD and Testing Strategy&lt;/h2&gt;

&lt;p&gt;To ensure swift rollout of new features and bug fixes, we use. Jenkins is Full ()
continuous integration and delivery approach using Jenkins. Beyond usual unit
tests Parameter changes or even more profound algorithmic updates to our
estimation algorithm. The only way to test such changes with. Important is to
not overoptimize for a particular groundtruth dataset, but to find a good
balance between groundtruth fit for different datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;groundtruth_fit1.png&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;Our grundtruth data  includes a plethora of data from real game data, to jogging
sessions where we have determined true distances, to high-precicion estimates of
GNSS positions and velocities, acquired with an expensive RTK GNSS receiver, to
light barrier data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;groundtruth_fit2.png&#34; width=800&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I will also touch upon some other aspects, like our
groundtruth based evaluation approaches and our growing from a few early
adopters after our product launch in 2017 to by now over 10000 active users
all over Germany, Switzerland and Austria, thus becoming the market leader in the DACH region.&lt;/p&gt;

&lt;!---

&gt;VERSION CONTROL WORKFLOW (ONEFLOW)
&gt;VERSIONING APPROACH
&gt;SCREENSHOT OF JUPYTER NOTEBOOK EVALUATION


## Future Work

- Machine-learning based metrics: Our
- Platform agnosticity: While our data analytics platform is closely intervoven 
  with various services (e.g. AWS Lambda, S3, ECS, SQS). Yet, in light of future 
  migration to different. This would require us to adopt, maybe on top of the 
  OpenStack datacenter operating system, and make our approach suitable for more 
  applications. Especially. - Towards stream processing. iSAM2, that however 
  requires to re-evaluate our algorithms. We have global parameters like the 
  earths magnetic field or sensor biases.
- Moving compute to the edge. At least parts of the processing pipeline could be 
  moved to the edge.
- Hot and cold storage of final results.
- More efficient storage formats.
- Spot instances

--&gt;

&lt;!---

&gt;DESCRIBE THE FACTOR GRAPH ALGORITHM AND PREPROCESSING
&gt;DESCRIBE OFFLINE VS ONLINE ESTIMATION
&gt;ADD FACTOR GRAPH SKETCH
&gt;SIMPLE IMU KITTI EXAMPLE FACTOR GRAPH

--&gt;

&lt;!---
&gt;WE TEAMED UP WITH SENSOR FUSION EXPERTS FROM KNOWTION
--&gt;

&lt;!-- I will also touch upon some other aspects, like our  --&gt;

&lt;!-- groundtruth based evaluation approaches and our growing from a few early  --&gt;

&lt;!-- adopters after our product launch in 2017 to by now over 10000 active users  --&gt;

&lt;!-- all over Germany, Switzerland and Austria, thus becoming the market leader in the DACH region. --&gt;

&lt;!-- developing techniques to build up a database of  --&gt;

&lt;!-- groundtruth and associated evaluation and testing approaches, and finally,  --&gt;

&lt;!-- setting up a highly-available and fault tolerant data analysis pipeline  --&gt;

&lt;!-- (&#34;pipeline&#34; in the following) - interestingly enough,  --&gt;

&lt;!-- not to speak of growing from a few early adopters after our product launch in 2017 to by  --&gt;

&lt;!-- now over 10000 active users all over Germany, Switzerland and Austria, thus  --&gt;

&lt;!-- becoming the market leader in the DACH region. --&gt;

&lt;!-- Before I joined TRACKTICS Tech team in 2016, TRACKTICS had already developed  --&gt;

&lt;!-- a powerful GNSS-IMU based sensor hardware tailored for the for the logging of  --&gt;

&lt;!-- movements of a football player during game or training sessions, a first version  --&gt;

&lt;!-- of a Web App that allowed one to visualize basic metrics inferred using an Extended  --&gt;

&lt;!-- Kalman Filter (executed in an offline manner) to combine the GNSS IMU and --&gt;

&lt;!-- Compass readings from the tracker. Yet, the estimation results of this early  --&gt;

&lt;!-- incarnation of the system was not sufficient in terms of estimation quality and  --&gt;

&lt;!-- there existed significant &#34;blind-spots&#34; as to how accurate the system was --&gt;

&lt;!-- operating overall, due to a still rather basic testing and evaluation framework.  --&gt;

&lt;!-- To solve this challenge we embarked on a very interesting journey of months of  --&gt;

&lt;!-- optimizing our algorithms, developing techniques to build up a database of  --&gt;

&lt;!-- groundtruth and associated evaluation and testing approaches, and finally,  --&gt;

&lt;!-- setting up a highly-available and fault tolerant data analysis pipeline  --&gt;

&lt;!-- (&#34;pipeline&#34; in the following) - interestingly enough, not to speak of  --&gt;

&lt;!-- growing from a few early adopters after our product launch in 2017 to by  --&gt;

&lt;!-- now over 10000 active users all over Germany, Switzerland and Austria, thus  --&gt;

&lt;!-- becoming the market leader in the DACH region. --&gt;

&lt;!-- Yet, the estimation results of this early --&gt;

&lt;!-- incarnation of the system was not sufficient in terms of estimation quality and  --&gt;

&lt;!-- there existed significant &#34;blind-spots&#34; as to how accurate the system was --&gt;

&lt;!-- operating overall, due to a still rather basic testing and evaluation framework.  --&gt;
</description>
    </item>
    
    <item>
      <title>Deep Q-Learning with PyTorch and the Unity ML-Agents Toolkit</title>
      <link>/post/deep-reinforcement-learning/</link>
      <pubDate>Mon, 18 Mar 2019 23:05:40 +0100</pubDate>
      
      <guid>/post/deep-reinforcement-learning/</guid>
      <description>&lt;!---

## Introduction
- How I got interested in the topic: The AI &amp; Games conference
--&gt;

&lt;p&gt;Coming soon &amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Animating data with Unity</title>
      <link>/project/football-animation/</link>
      <pubDate>Fri, 28 Dec 2018 11:27:00 +0100</pubDate>
      
      <guid>/project/football-animation/</guid>
      <description>&lt;p&gt;At &lt;a href=&#34;https://www.tracktics.com&#34; target=&#34;_blank&#34;&gt;TRACKTICS&lt;/a&gt; we are constantly trying to come up with new ways to visualize the
data that our customers acquire with their GPS Performance Trackers. Recently, I
created a small prototype, visualizing locations and orientation (yaw angle) time
series data in the form of three-dimensional animations using the Unity 3D engine. Here&amp;rsquo;s a small example:&lt;/p&gt;

&lt;iframe
src=&#34;https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2Ftracktics%2Fvideos%2F1073790999445342%2F&amp;show_text=0&amp;width=476&#34;
width=&#34;476&#34; height=&#34;476&#34; style=&#34;border:none;overflow:hidden&#34; scrolling=&#34;no&#34;
frameborder=&#34;0&#34; allowTransparency=&#34;true&#34; allowFullScreen=&#34;true&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;The visualization has been featured in a Report by &lt;a href=&#34;https://reportage2.stuttgarter-zeitung.de/fussball-digitalisierung&#34; target=&#34;_blank&#34;&gt;Stuttgarter
Zeitung&lt;/a&gt;
and might be turned into a new feature for TRACKTIC&amp;rsquo;s &lt;a href=&#34;https://app.tracktics.com&#34; target=&#34;_blank&#34;&gt;Web
App&lt;/a&gt; feature - not decided yet, though ;-).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data analysis pipeline</title>
      <link>/project/data-pipelines/</link>
      <pubDate>Fri, 28 Dec 2018 11:27:00 +0100</pubDate>
      
      <guid>/project/data-pipelines/</guid>
      <description>&lt;p&gt;The TRACKTICS Pipeline is a framework that allows to orchestrate parallel DAG
(Direct Acyclic Graphs) batch processing jobs. It is based on Docker for
artifact deployment, ECS (Amazon EC2 Container Service) for container
orchestration and resource discovery, Luigi (by Spotify) for workflow and
dependency management, SQS (Amazon Simple Queuing System) as a job queue, a
custom service for auto scaling, S3 and Postgres (Amazon RDS) to store
results and Kinesis for logging. On a service level our toolbox features a
universal Python wrapper that allows to expose the functionality of each
service to ECS and the workflow manager, in a consistent way. I have written
a &lt;a href=&#34;http://auerl.github.io/post/tracktics-data-pipeline/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; post
about how we developed our pipeline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Global seismic anisotropy</title>
      <link>/project/global-anisotropy/</link>
      <pubDate>Fri, 28 Dec 2018 11:27:00 +0100</pubDate>
      
      <guid>/project/global-anisotropy/</guid>
      <description>

&lt;h2 id=&#34;global-anisotropy-savani&#34;&gt;Global anisotropy (SAVANI)&lt;/h2&gt;

&lt;p&gt;Together with colleagues at UPMC Paris (&lt;a href=&#34;http://hestia.lgs.jussieu.fr/~boschil/&#34; target=&#34;_blank&#34;&gt;Lapo
Boschi&lt;/a&gt;), USC Los Angeles
(&lt;a href=&#34;http://www-udc.ig.utexas.edu/external/becker/&#34; target=&#34;_blank&#34;&gt;Thorsten Becker&lt;/a&gt;) and
University of Oxford (&lt;a href=&#34;http://seis.earth.ox.ac.uk/people/tarje.html&#34; target=&#34;_blank&#34;&gt;Tarje
Nissen-Meyer&lt;/a&gt;),
have developed a new global model of anisotropic shear-velocity variations, called SAVANI,
which represents one of the first attempts to map anisotropic wavespeeds at the
scale of the entire mantle. While being well correlated with earlier models at
long spatial wavelength, our preferred solution, savani, additionally delineates
a number of previously unidentified structures, due to its improved resolution
in areas of dense coverage.&lt;/p&gt;

&lt;p&gt;This is because the density of the inverse grid ranges between 1.25° in well
sampled to 5° in poorly sampled regions, allowing us to resolve regional
structure better than it is typically the case in global S-wave tomography.
Important features of our model include: (i) A distinct ocean-continent
anisotropic signature in the uppermost mantle; (ii) an oceanic peak in above
average ξ &amp;gt; 1 which is shallower than in previous models and thus in better
agreement with estimates of lithosphere thickness.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;age-dependency.jpg&#34; width=500&gt;&lt;/p&gt;

&lt;p&gt;You can find more details about SAVANI and its construction in &lt;a href=&#34;https://doi.org/10.1002/2013JB010773&#34; target=&#34;_blank&#34;&gt;Auer et al. 2014&lt;/a&gt;. Also, feel free to
download our model and documentation &lt;a href=&#34;https://github.com/auerl/savani&#34; target=&#34;_blank&#34;&gt;from my
GitHub&lt;/a&gt;, if you want to use the model for
your own work.&lt;/p&gt;

&lt;h2 id=&#34;oceanic-upper-mantle&#34;&gt;Oceanic upper mantle&lt;/h2&gt;

&lt;p&gt;Defining the oceanic lithosphere as a thermal boundary layer allows to explain,
to first order, age-dependent bathymetry and isotropic wavespeeds. In contrast,
SS precursors and receiver functions suggest a subhorizontal interface within
this layer, on top of a radially anisotropic zone. Comparing a suite of
geodynamic scenarios against surface-wave dispersion data and seismic
discontinuties we find that only weak age dependency of the radially anisotropic
zone is compatible with observations. We show that this zone is confined from
below by a second, weaker seismic interface.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;upper-mantle.png&#34; width=500&gt;&lt;/p&gt;

&lt;p&gt;While azimuthal anisotropy is consistent with LPO of olivine due to
asthenospheric flow underneath the lithosphere, radial anisotropy requires
additional contributions, perhaps from petrological fabrics or melt ponding.
This implies that seismic reflectors previously associated with the base of the
lithosphere are instead associated with preserved structures embedded in it.
They carry information about plate formation, but have little control on plate
deformation. See &lt;a href=&#34;https://doi.org/10.1002/2015GL066246&#34; target=&#34;_blank&#34;&gt;Auer et al. 2015&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;software-for-global-tomography&#34;&gt;Software for global tomography&lt;/h2&gt;

&lt;p&gt;During my PhD I developed and contributed to several tools for global
tomographic imaging, some of which are publicly available. See:&lt;/p&gt;

&lt;h4 id=&#34;petscinv-a-general-purpose-parallel-tomography-solver-based-on-petsc&#34;&gt;PETScinv - A general purpose parallel tomography solver based on PETSc&lt;/h4&gt;

&lt;p&gt;PETScinv is a modular, general purpose tomography solver written in (object
oriented) Fortran 2008. PETScinv solves global tomographic imaging problems
in parallel using the &lt;a href=&#34;https://www.mcs.anl.gov/petsc/&#34; target=&#34;_blank&#34;&gt;PETSc&lt;/a&gt; component
&lt;a href=&#34;https://www.mcs.anl.gov/petsc/petsc-current/docs/manualpages/KSP/index.html&#34; target=&#34;_blank&#34;&gt;KSP&lt;/a&gt;.
The scalable linear equations solvers
(KSP) module provides an easy-to-use interface to the combination of a Krylov
subspace iterative method and a preconditioner (in the KSP and PC components,
respectively) or a sequential direct solver.&lt;/p&gt;

&lt;p&gt;The code currently only supports orthogonal, curvilinear hexahedral basis
functions (spherical sections or voxels), but adding modules to treat other
parametization strategies (splines, spherical harmonics, wavelets) to discretize
the model space, should be straightforward. One can account for different
regularization approaches, such as roughness minimization or norm minimization.
Tomography results can be exported in a simple ASCII format or as netcdf files,
together with an .xdmf file, ready to be explored with ParaView. Get the code
via my &lt;a href=&#34;https://github.com/auerl/petscinv.git&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;flexinv-ray-theoretical-surface-and-body-wave-sensitivity-kernels&#34;&gt;FLEXinv - Ray-theoretical surface and body wave sensitivity kernels&lt;/h4&gt;

&lt;p&gt;Flexinv is a Fortran/Python toolbox to create global, adaptive resolution
transversely isotropic tomographic models of the entire mantle, via joint
inversion of surface-wave dispersion and body-wave traveltimes, in the
high-frequency ray approximation. See &lt;it&gt;Auer et al. (2014)&lt;/it&gt; and &lt;it&gt;Boschi (2009)&lt;/it&gt;
for a complete description of the algorithm.&lt;/p&gt;

&lt;p&gt;Some components of the surface wave parts of this code go back to &lt;em&gt;Woodhouse
(1981)&lt;/em&gt; and &lt;em&gt;Dziewonski &amp;amp; Anderson (1981)&lt;/em&gt;. The body-wave routines go back to
&lt;em&gt;Gu (2005)&lt;/em&gt;. Crustal corrections are based on CRUST2.0 by &lt;em&gt;Laske et al. (2002)&lt;/em&gt;.
Models are parameterized in curvilinear hexahedrons, whose size is adapted
to local ray coverage, following &lt;em&gt;Schäfer et al. (2011)&lt;/em&gt;. Get the code
via my &lt;a href=&#34;https://github.com/auerl/flexinv.git&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;mckernel-broadband-waveform-sensitivity-kernels-for-seismic-tomography&#34;&gt;MCkernel - Broadband waveform sensitivity kernels for seismic tomography&lt;/h4&gt;

&lt;p&gt;MCkernel is a software implementation to calculate seismic sensitivity kernels
on arbitrary tetrahedral or hexahedral grids across the whole observable seismic
frequency band. These kernels rely on wavefield databases computed via AxiSEM
(www.axisem.info), and thus on spherically symmetric models. The advantage is
that frequencies up to 0.2 Hz and higher can be accessed.&lt;/p&gt;

&lt;p&gt;Since the usage of irregular, adapted grids is an integral part of
regularisation in seismic tomography, MCkernel works in a inversion-grid-centred
fashion: A Monte-Carlo integration method is used to project the kernel onto
each basis function, which allows to control the desired precision of the kernel
estimation.&lt;/p&gt;

&lt;p&gt;Also, it means that the code concentrates calculation effort on regions of
interest without prior assumptions on the kernel shape. The code makes extensive
use of redundancies in calculating kernels for different receivers or
frequency-pass-bands for one earthquake, to facilitate its usage in large-scale
global seismic tomography. MCkernel is available &lt;a href=&#34;http://seismology.github.io/mc_kernel/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sprint age dependency</title>
      <link>/project/sprint-age-dependency/</link>
      <pubDate>Fri, 28 Dec 2018 11:27:00 +0100</pubDate>
      
      <guid>/project/sprint-age-dependency/</guid>
      <description>

&lt;h2 id=&#34;age-dependency-of-sprint-top-speeds&#34;&gt;Age-dependency of sprint top speeds&lt;/h2&gt;

&lt;p&gt;TRACKTICS is having a long-term partnership with Fundación Real Madrid Clinics
(FRMC) where we equip hundreds of football schools under the Real Madrid Brand
Umbrella with our GPS performance trackers. In the scope of this partnership we have
collected performance data of tens of thousands of youth players, which amounts
to what we think is probably the largest data set of its kind.&lt;/p&gt;

&lt;p&gt;Based on the Real Madrid dataset we have devised a new model of sprint top speed
age-dependency, using simple restricted cubic spline regression (see the figure above).
While we have used the resulting model in the scope of our partnership with FRMC
we have not yet got the time to wrap up the model in a publication, which would
be interesting for a larger audience, as our model is probably the most
extensive modelling attempt of sprint top speed age-dependency ever conducted.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Twitter sentiment analysis</title>
      <link>/project/cloudquake/</link>
      <pubDate>Fri, 28 Dec 2018 11:27:00 +0100</pubDate>
      
      <guid>/project/cloudquake/</guid>
      <description>&lt;p&gt;CloudQuake is a university project where I tried to apply real-time sentiment analysis on Twitter
streams. Our approach was to first collect data via Amazon Kinesis (which allows
for a convenient way to combine data from different sources) and then use Apache
Spark Realtime and its MLlib to classify tweets using a simple Naive Bayes
classifier trained with publicly available annotations. While the initial idea
was to first filter the data for earthquake related tweets and then combine the
inferred tweet sentiments with a Long/Short term moving average indicator with
the goal to detect whether the mentions of Quake-related tweets allow to detect
earthquakes. The detection didn&amp;rsquo;t prove to be stable probably owed to the
relatively limited amount of tweets that is provided by the Twitters free Api.
So all is left is a nice Twitter sentiment analysis engine ;-). The code is
&lt;a href=&#34;https://github.com/auerl/cqsent&#34; target=&#34;_blank&#34;&gt;available&lt;/a&gt; on my GitHub.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Waveform Tomography</title>
      <link>/project/waveform-imaging/</link>
      <pubDate>Fri, 28 Dec 2018 11:27:00 +0100</pubDate>
      
      <guid>/project/waveform-imaging/</guid>
      <description>

&lt;h2 id=&#34;full-waveform-tomography&#34;&gt;Full Waveform Tomography&lt;/h2&gt;

&lt;p&gt;Seismic Full Waveform Tomography has been one of my main interests since my
thesis supervisor Prof. Stewart Greenhalgh has introduced me to the
topic during the final year of my Master&amp;rsquo;s studies in 2011. Full waveform
tomography describes an inverse modeling approach, where information encoded in
subtle variations in seismic and acoustic waveforms that have traversed
complex media, are mapped back into structural images of the subsurface at
scales from medical imaging applications up to dimensions of the whole earth.&lt;/p&gt;

&lt;p&gt;At the core of these methods lie, on one hand, compute intensive forward problems
where the seismic or acoustic wave equation is solved to compute &amp;ldquo;sensitivity
kernels&amp;rdquo; (essentially reverse mapping functions from waveform or traveltime
differences to structure) as well as a large-scale optimization problems, where
the sensitivity kernels are used to reconstruct structural models of wave
speeds or other physical parameters.&lt;/p&gt;

&lt;p&gt;There have been a plethora of approaches to solve these problems, pursuing
different strategies to compute sensitivity kernels (time domain, frequency
domain, approximate methods, full-wave methods) and to solve the inverse problem
(conjugate gradient methods, hessian based methods, &amp;hellip;).&lt;/p&gt;

&lt;p&gt;While during my &lt;a href=&#34;https://auerl.github.io/downloads/msc_thesis.pdf&#34; target=&#34;_blank&#34;&gt;MSc thesis&lt;/a&gt; I
worked on frequency-domain waveform inversion on the exploration (and near surface)
scale, during my &lt;a href=&#34;https://doi.org/10.3929/ethz-a-010715830&#34; target=&#34;_blank&#34;&gt;PhD thesis&lt;/a&gt; I focused on
methods for whole-earth imaging. In this context I have developed a waveform
based imaging method for global tomographiy imaging that is called &amp;ldquo;Broadband
Tomography&amp;rdquo; and combines infinite-frequency ray theory for the surface wave
portions of the seismogram, with finite-frequency kernels for the body wave
portion of the seismic record.&lt;/p&gt;

&lt;h2 id=&#34;3d-to-2d-data-transformations&#34;&gt;3D-to-2D Data Transformations&lt;/h2&gt;

&lt;p&gt;My first project in the area of full waveform tomography has been during my MSc
thesis where I was investigating detrimental effect of common 3D-to-2D data
transformations on frequency domain FWI. This is important because seismic full
waveform inversion is often  based on forward modelling in the
computationally attractive 2-D domain. Any solution of the 2-D cartesian wave
equation inherently carries the assumption of being generated by a line source,
extended in the out-of-plane direction, implying that source energy spreads over
the surface of a cylinder, and that amplitudes scale with one over square-root
of distance. However, realistic point sources like explosives or airguns, fired
in a 3-D medium, generate amplitudes that decay with one over distance, since
the wavefield expands spherically in all three dimensions.&lt;/p&gt;

&lt;p&gt;Usually, geophysicists correct for this amplitude difference and the associated
phase shift of π/4 by transforming recorded field data to approximate 2-D, using
simplistic asymptotic filter algorithms, operating on a time-sample basis and
assuming straight ray paths and a constant velocity medium. The sometimes
careless usage of these filters, is in contradiction to their well known
limitations. Comparing simulated data using a FDTD forward solver
for visco-elastic seismic wave propagation in 3D and 2D media, we could show
that filtering errors are moderate in purely acoustic situations but become
substantial in complex media when arrivals overlap each
other or ray paths deviate strongly from straight lines. Normalized
root-mean-square deviations up to 5% and maximum relative time domain errors up
to 40% were found in high contrast media, when full elastic treatment is
considered. We published our results in &lt;a href=&#34;https://doi.org/10.1190/geo2012-0382.1&#34; target=&#34;_blank&#34;&gt;Auer et al. 2013&lt;/a&gt;.
Also feel free to download my &lt;a href=&#34;https://auerl.github.io/downloads/msc_thesis_auer.pdf&#34; target=&#34;_blank&#34;&gt;thesis&lt;/a&gt;
or a pdf of my &lt;a href=&#34;https://auerl.github.io/downloads/msc_talk_auer.pdf&#34; target=&#34;_blank&#34;&gt;thesis presentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;2-5-d-full-waveform-tomography&#34;&gt;2.5-D Full Waveform Tomography&lt;/h2&gt;

&lt;p&gt;Motivated by the questionable performance of the aforementioned 3D-to-2D filters
the second chapter of my thesis investiagted potential of 2.5-D frequency-domain
finite-element modelling of seismic wave propagation, which is based on Fourier
transforming the 3-D wave equation along the invariant axis to
frequency-wavenumber  domain and solving the resulting equation for many
wavenumber components, thus breaking down the 3-D problem to a large number of
2-D problems. In my thesis I made an attempt to circumvent problems associated
with singularities in the wavenumber spectrum by combining a finite-element
2.5-D forward solver and a complex-frequency extension.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;wavenumber-spectrum.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;By shifting the poles off the real axis, they are avoided in the inverse Fourier
transform to the frequency domain. I could show that problems associated
with the singularities are mitigated and the total
number of required wavenumber samples is significantly reduced, as soon as
frequencies are allowed to be complex. Reconstructed finite-element 2.5-D
seismograms compare very well to reference finite-difference 3-D seismograms.
2.5-D modelling with complex frequencies outperforms asymptotic 3D-to-2D
transformation showing very low misfits between 2.5-D and 3-D synthetics.&lt;/p&gt;

&lt;h2 id=&#34;broadband-finite-frequency-kernels&#34;&gt;Broadband Finite Frequency Kernels&lt;/h2&gt;

&lt;p&gt;Unlike at the exploration scale, tomographic imaging of at the dimension of
the whole mantle is with few exceptions still using ray theoretic methods.
In my &lt;a href=&#34;https://doi.org/10.3929/ethz-a-010715830&#34; target=&#34;_blank&#34;&gt;PhDthesis&lt;/a&gt; I have presented
the theoretical and methological foundations of a new global
waveform tomography approach, with the rationale to keep approximate (ray)
theory where appropriate (global long-wavelength structure, surface
wavedispersion), but to revert to a full-waveform interpretation where
necessary (regional scale, non-geometrical wave phenomena).
Finite-frequency kernels are being calculated via the spectral element solver
&lt;a href=&#34;https://github.com/geodynamics/axisem&#34; target=&#34;_blank&#34;&gt;AxiSEM&lt;/a&gt; (Nissen-Meyer et al. 2014) and
a new code called &lt;a href=&#34;http://seismology.github.io/mc_kernel/&#34; target=&#34;_blank&#34;&gt;MCkernel&lt;/a&gt;.
Our imaging method provides a framework to stepwise improve upon a global
background model by re-inverting an updated linear system whenever new (types
of) data become available.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;waveform-kernels.png&#34; width=500&gt;&lt;/p&gt;

&lt;p&gt;The huge advantage of using numerical methods to compute banana donought kernels
as shown in the above figure is that while wave theory breaks down for
non-geometrical wave phenomena like Core-diffractions, spectral-element based FF
kernels can account for the complex interactions at the core mantle boundary and
thus allow imaging those interesting regions of the earths mantle with higher
accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;predicted-deltat.png&#34; width=500&gt;&lt;/p&gt;

&lt;p&gt;My PhD thesis can be downloaded
&lt;a href=&#34;https://doi.org/10.3929/ethz-a-010715830&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.
Also find my thesis presentation under &lt;a href=&#34;https://auerl.github.io/downloads/phd_talk_auer.pdf&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Thermal structure, radial anisotropy, and dynamics of oceanic boundary layers</title>
      <link>/publication/doi-10-1002-2015-gl-066246/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0100</pubDate>
      
      <guid>/publication/doi-10-1002-2015-gl-066246/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AxiSEM: broadband 3-D seismic wavefields in axisymmetric media</title>
      <link>/publication/doi-10-5194-se-5-425-2014/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0100</pubDate>
      
      <guid>/publication/doi-10-5194-se-5-425-2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mantle dynamics in the Mediterranean</title>
      <link>/publication/doi-10-1002-2013-rg-000444/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0100</pubDate>
      
      <guid>/publication/doi-10-1002-2013-rg-000444/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Savani: A variable resolution whole-mantle model of anisotropic shear velocity variations based on multiple data sets</title>
      <link>/publication/doi-10-1002-2013-jb-010773/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0100</pubDate>
      
      <guid>/publication/doi-10-1002-2013-jb-010773/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A critical appraisal of asymptotic 3D-to-2D data transformation in full-waveform seismic crosshole tomography</title>
      <link>/publication/doi-10-1190-geo-2012-0382-1/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0100</pubDate>
      
      <guid>/publication/doi-10-1190-geo-2012-0382-1/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
