<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on https://auerl.github.io</title>
    <link>/tags/docker/</link>
    <description>Recent content in Docker on https://auerl.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Mon, 18 Mar 2019 23:05:47 +0100</lastBuildDate>
    
	<atom:link href="/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Building a scalable batch processing pipeline for sensor fusion using Docker, Luigi, ECS and S3</title>
      <link>/post/tracktics-data-pipeline/</link>
      <pubDate>Mon, 18 Mar 2019 23:05:47 +0100</pubDate>
      
      <guid>/post/tracktics-data-pipeline/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data analysis pipeline</title>
      <link>/project/data-pipelines/</link>
      <pubDate>Fri, 28 Dec 2018 11:27:00 +0100</pubDate>
      
      <guid>/project/data-pipelines/</guid>
      <description>The TRACKTICS Pipeline is a framework that allows to orchestrate parallel DAG (Direct Acyclic Graphs) batch processing jobs. It is based on Docker for artifact deployment, ECS (Amazon EC2 Container Service) for container orchestration and resource discovery, Luigi (by Spotify) for workflow and dependency management, SQS (Amazon Simple Queuing System) as a job queue, a custom autoscaler, S3 and Postgres (Amazon RDS) to store results and Kinesis to for logging. On a service level our toolbox features a universal Python wrapper that allows to expose the functionality of each service to ECS and the workflow manager, in a consistent way.</description>
    </item>
    
  </channel>
</rss>